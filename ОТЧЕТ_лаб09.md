# Отчет по лабораторной работе №9  
# Репликация и отказоустойчивость

## Сведения о студенте  

**Дата:** 13.12.2025  
**Семестр:** 7 семестр  
**Группа:** ПИЖ-Б-О-22-1  
**Дисциплина:** Администрирование баз данных  
**Студент:** Магомедов Имран Борисович  

## Цель работы  

Освоить настройку и управление физической и логической репликацией в PostgreSQL 16, включая создание и использование слотов репликации, настройку синхронного и асинхронного режимов, а также каскадной репликации.
Изучить процедуры переключения при отказе основного сервера (failover) и базовые концепции построения отказоустойчивых кластеров PostgreSQL.

## Теоретическая часть  

### Изученные концепции  

- **Физическая репликация:** Передача и применение WAL‑журналов с основного сервера на один или несколько резервных серверов, что формирует побайтовую копию всего кластера БД и используется для высокой доступности и защиты от аппаратных сбоев.

- **Логическая репликация:** Репликация на уровне изменений данных в таблицах (INSERT/UPDATE/DELETE) через публикации и подписки, позволяющая выборочно реплицировать отдельные таблицы и использовать разные версии PostgreSQL или разные платформы на ведущем и ведомых серверах.

- **Синхронная и асинхронная репликация:** В синхронном режиме фиксация транзакции на мастере ожидает подтверждения записи на по крайней мере одной реплике, минимизируя риск потери данных, тогда как в асинхронном режиме возможен отрыв реплики и потеря части последних транзакций при аварийном переключении.

- **Слоты репликации и обратная связь:** Слот репликации фиксирует позицию чтения WAL подписчиком или стендбаем и предотвращает преждевременную очистку нужных журналов, а механизм обратной связи позволяет передавать на основной сервер информацию о прогрессе применения изменений и удерживать необходимые версии строк.

- **Failover и каскадная репликация:** Failover предполагает перевод одной из реплик в роль основного сервера при отказе мастера, при этом каскадная репликация позволяет строить цепочки стендбаев, где часть реплик получает WAL не от мастера напрямую, а от других реплик для разгрузки основного узла и повышения масштабируемости.

### Ключевые термины  

- **Мастер (primary) и реплика (standby):** Основной сервер принимает записи и генерирует WAL, а реплики применяют журнал и обычно используются для чтения или резервирования.

- **WAL (Write-Ahead Log):** Журнал предзаписи, в котором фиксируются все изменения данных и который является основой для физической и логической репликации.

- **Hot Standby:** Режим реплики, в котором она применяет WAL и одновременно принимает пользовательские чтения в режиме только для чтения, что позволяет разгружать основной сервер.

- **Кластер PostgreSQL:** Набор процессов сервера и файлов данных одного экземпляра PostgreSQL, который может быть частью отказоустойчивой архитектуры с несколькими узлами и механизмами автоматического или ручного переключения.

## Практическая часть

### Модуль 1: Физическая репликация

#### Задача 1: Базовая настройка

**Цель:** Настроить физическую потоковую репликацию PostgreSQL 16 между двумя экземплярами сервера (на разных портах) в синхронном режиме, проверить копирование данных и поведение фиксации транзакций при остановке реплики.

**Выполненные действия:**

```bash
# 0. Подготовка каталогов данных (если ещё нет)
mkdir -p /home/student/pg16_primary
mkdir -p /home/student/pg16_standby

chmod 700 /home/student/pg16_primary
chmod 700 /home/student/pg16_standby

# 1. Инициализация первичного кластера (мастера)
initdb -D /home/student/pg16_primary

# 2. Настройка postgresql.conf на мастере (порт, WAL, репликация)
# (открыть файл любым редактором, nano/vim)
nano /home/student/pg16_primary/postgresql.conf
# Добавить/изменить строки:
# port = 5432
# wal_level = replica
# max_wal_senders = 5
# max_replication_slots = 5
# synchronous_commit = on
# synchronous_standby_names = 'standby1'
# hot_standby = off        # на мастере это значение не важно, но можно оставить по умолчанию

# 3. Настройка доступа для репликации (pg_hba.conf на мастере)
nano /home/student/pg16_primary/pg_hba.conf
# Добавить строки в конец файла:
# Разрешаем подключение репликации от локального пользователя
# (можно оставить только одну строку, если используешь local)
host    replication    repuser     127.0.0.1/32        md5
local   replication    repuser                         trust

# 4. Создание роли для репликации
pg_ctl -D /home/student/pg16_primary -l /home/student/pg16_primary/logfile start
```

```sql
-- Primary (-U student -p 5432 -D postgres) 
CREATE ROLE repuser WITH REPLICATION LOGIN PASSWORD 'rep_pass';
```

```bash
# 5. Создание базовой копии для реплики (standby) через pg_basebackup
pg_basebackup \
  -h 127.0.0.1 \
  -p 5432 \
  -U repuser \
  -D /home/student/pg16_standby \
  -Fp \
  -X stream \
  -P

# 6. Настройка реплики (standby) - порт и параметры
nano /home/student/pg16_standby/postgresql.conf
# Изменить/добавить:
# port = 5433
# wal_level = replica         # можно оставить значение из basebackup
# hot_standby = on
# primary_conninfo = 'host=127.0.0.1 port=5432 user=repuser password=rep_pass application_name=standby1'
# primary_slot_name = 'standby1_slot'   # если хочешь использовать слот
```

```sql
-- Primary (-U student -p 5432 -D postgres) 
-- 7. (Опционально) создать слот репликации на мастере
SELECT * FROM pg_create_physical_replication_slot('standby1_slot');
```

```bash
# 8. Создать файл standby.signal на реплике
touch /home/student/pg16_standby/standby.signal

# 9. Запуск реплики
pg_ctl -D /home/student/pg16_standby -l /home/student/pg16_standby/logfile start
```

```sql
-- Primary (-U student -p 5432 -D postgres) 
-- 10. Проверка состояния репликации на мастере
SELECT application_name,
       client_addr,
       state,
       sync_state
FROM pg_stat_replication;

-- 11. Проверка копирования данных:
-- На мастере создаём тестовую базу/таблицу и вставляем данные
CREATE DATABASE repl_test;
\c repl_test

CREATE TABLE test_data (
    id   serial PRIMARY KEY,
    info text
);
INSERT INTO test_data (info) VALUES ('row1'), ('row2');
SELECT * FROM test_data ORDER BY id;
```

```sql
-- Standby (-U student -p 5433 -D repl_test) 
-- На реплике проверяем наличие БД и данных (режим только чтение)
SELECT * FROM test_data ORDER BY id;
```

```bash
# 12. Проверка синхронного режима:
# 12.1. Останавливаем реплику
pg_ctl -D /home/student/pg16_standby stop
```

```sql
-- Primary (-U student -p 5432 -D repl_test) 
-- 12.2. На мастере пытаемся зафиксировать транзакцию
BEGIN;
INSERT INTO test_data (info) VALUES ('blocked_row');
COMMIT;
-- (команда COMMIT должна «висеть» и не завершаться, пока реплика остановлена)
```

```bash
# 12.3. В другом терминале снова запускаем реплику
pg_ctl -D /home/student/pg16_standby -l /home/student/pg16_standby/logfile start

# После старта реплики COMMIT на мастере должен завершиться,
# а новая строка появится на реплике.
```

**Результаты:**

```bash
# 10. Проверка состояния репликации на мастере
 application_name | client_addr |   state   | sync_state 
------------------+-------------+-----------+------------
 standby1         | 127.0.0.1   | streaming | sync
(1 row)

# На мастере
 id |    info     
----+-------------
  1 | row1
  2 | row2
(2 rows)

# На реплике
 id |    info     
----+-------------
  1 | row1
  2 | row2
(2 rows)
```

**Выводы и объяснения:**

В результате настройки потоковой физической репликации между кластерами на портах 5432 и 5433 реплика подключилась в режиме streaming со статусом sync, что подтверждает успешную работу синхронной репликации.​

Созданная на мастере база repl_test и таблица test_data с двумя строками сразу стали доступны на реплике в режиме только чтения, что показывает корректную передачу WAL-записей и применение изменений на стендбае.​

При остановке реплики команда COMMIT на мастере «зависает» до повторного запуска стендбая, потому что в синхронном режиме фиксация транзакций завершается только после подтверждения реплики; после старта стендбая транзакция успешно фиксируется и новая строка появляется на реплике, что демонстрирует семантику синхронного коммита и отсутствие потери данных.​

#### Задача 2: Конфликты применения

**Цель:** Показать, как параметр max_standby_streaming_delay и настройка hot_standby_feedback влияют на конфликты между длительными запросами на реплике и операциями VACUUM на мастере.​

**Выполненные действия:**

```sql
-- 1. Подготовка тестовой таблицы (на мастере, порт 5432)
DROP TABLE IF EXISTS conflict_test;
CREATE TABLE conflict_test (
    id   serial PRIMARY KEY,
    info text
);

INSERT INTO conflict_test (info)
SELECT 'row-' || g
FROM generate_series(1, 200000) AS g;

ANALYZE conflict_test;
```

```sql
--- 2. На реплике включаем hot_standby и уменьшаем задержку применения WAL
---    Сначала эксперимент без обратной связи, когда конфликтующий SELECT будет прерван.
ALTER SYSTEM SET max_standby_streaming_delay = '-1';
ALTER SYSTEM SET hot_standby_feedback = off;
SELECT pg_reload_conf();
```

```sql
-- 3. На реплике запускаем долгий SELECT (от student на порту 5433)
-- Отдельная сессия (реплика):
SELECT pg_backend_pid();

SELECT count(*)
FROM conflict_test, pg_sleep(10)
WHERE conflict_test.id <= 150000;
-- Запрос должен выполняться заметное время.
```

```sql
-- 4. На мастере параллельно запускаем VACUUM той же таблицы
DELETE FROM conflict_test WHERE id <= 150000;
VACUUM (VERBOSE) conflict_test;
-- Через какое‑то время на реплике должен появиться ошибка
```

```sql
-- 5. Включаем hot_standby_feedback на реплике
ALTER SYSTEM SET hot_standby_feedback = on;
SELECT pg_reload_conf();
```

```sql 
-- 6. Повторяем эксперимент:
-- На реплике снова запускаем долгий SELECT
SELECT count(*)
FROM conflict_test, pg_sleep(10)
WHERE conflict_test.id <= 150000;
```

```sql
--- 7. На мастере снова запускаем VACUUM
DELETE FROM conflict_test WHERE id <= 150000;
VACUUM (VERBOSE) conflict_test;
-- Теперь SELECT на реплике должен успешно завершиться
```

**Результаты:**

```bash
# Реплика
 pg_backend_pid 
----------------
          97384
(1 row)

ERROR:  canceling statement due to conflict with recovery
DETAIL:  User query might have needed to see row versions that must be removed.

# Мастер
INFO:  vacuuming "repl_test.public.conflict_test"
INFO:  finished vacuuming "repl_test.public.conflict_test": index scans: 1
pages: 0 removed, 1082 remain, 1082 scanned (100.00% of total)
tuples: 150000 removed, 50000 remain, 0 are dead but not yet removable
removable cutoff: 813, which was 0 XIDs old when operation ended
new relfrozenxid: 810, which is 1 XIDs ahead of previous value
frozen: 0 pages from table (0.00% of total) had 0 tuples frozen
index scan needed: 811 pages from table (74.95% of total) had 150000 dead item identifiers removed
index "conflict_test_pkey": pages: 551 in total, 410 newly deleted, 410 currently deleted, 0 reusable
avg read rate: 0.000 MB/s, avg write rate: 1.305 MB/s
buffer usage: 5994 hits, 0 misses, 4 dirtied
WAL usage: 3936 records, 1 full page images, 1143726 bytes
system usage: CPU: user: 0.02 s, system: 0.00 s, elapsed: 0.02 s
INFO:  vacuuming "repl_test.pg_toast.pg_toast_16509"
INFO:  finished vacuuming "repl_test.pg_toast.pg_toast_16509": index scans: 0
pages: 0 removed, 0 remain, 0 scanned (100.00% of total)
tuples: 0 removed, 0 remain, 0 are dead but not yet removable
removable cutoff: 813, which was 0 XIDs old when operation ended
new relfrozenxid: 813, which is 4 XIDs ahead of previous value
frozen: 0 pages from table (100.00% of total) had 0 tuples frozen
index scan not needed: 0 pages from table (100.00% of total) had 0 dead item identifiers removed
avg read rate: 7.940 MB/s, avg write rate: 0.000 MB/s
buffer usage: 17 hits, 1 misses, 0 dirtied
WAL usage: 1 records, 0 full page images, 188 bytes
system usage: CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s

# Реплика
 pg_backend_pid 
----------------
          97384
(1 row)

 count  
--------
 150000
(1 row)

# Мастер
INFO:  vacuuming "repl_test.public.conflict_test"
INFO:  finished vacuuming "repl_test.public.conflict_test": index scans: 0
pages: 0 removed, 1082 remain, 1082 scanned (100.00% of total)
tuples: 0 removed, 200000 remain, 150000 are dead but not yet removable
removable cutoff: 814, which was 5 XIDs old when operation ended
frozen: 0 pages from table (0.00% of total) had 0 tuples frozen
index scan not needed: 0 pages from table (0.00% of total) had 0 dead item identifiers removed
avg read rate: 0.000 MB/s, avg write rate: 1.789 MB/s
buffer usage: 2170 hits, 0 misses, 3 dirtied
WAL usage: 1 records, 0 full page images, 188 bytes
system usage: CPU: user: 0.01 s, system: 0.00 s, elapsed: 0.01 s
INFO:  vacuuming "repl_test.pg_toast.pg_toast_16518"
INFO:  finished vacuuming "repl_test.pg_toast.pg_toast_16518": index scans: 0
pages: 0 removed, 0 remain, 0 scanned (100.00% of total)
tuples: 0 removed, 0 remain, 0 are dead but not yet removable
removable cutoff: 814, which was 5 XIDs old when operation ended
frozen: 0 pages from table (100.00% of total) had 0 tuples frozen
index scan not needed: 0 pages from table (100.00% of total) had 0 dead item identifiers removed
avg read rate: 59.637 MB/s, avg write rate: 0.000 MB/s
buffer usage: 17 hits, 1 misses, 0 dirtied
WAL usage: 1 records, 0 full page images, 188 bytes
system usage: CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s
```

**Выводы и объяснения:**

Параметр max_standby_streaming_delay задаёт, сколько времени реплика может откладывать применение WAL, прежде чем отменить конфликтующие запросы; при малом значении длительный SELECT на стендбае прерывается с ошибкой конфликта восстановления, если VACUUM на мастере пытается удалить нужные ему версии строк.​

Включение hot_standby_feedback = on приводит к тому, что реплика сообщает мастеру о минимальных нужных версиях строк, и VACUUM на мастере откладывает их удаление, поэтому запрос на реплике не прерывается, но цена за это - возможный рост «висячих» версий строк и увеличение объёма таблицы и задержки репликации.​

#### Задача 3: Слоты репликации

**Цель:** Показать, что физический слот репликации на мастере удерживает WAL‑файлы, пока реплика не прочитала их, и что после удаления слота удержание WAL прекращается и очистка возобновляется.​

**Выполненные действия:**

```sql
-- 1. На мастере смотрим текущий слот (создан ранее как standby1_slot)
SELECT slot_name,
       active,
       restart_lsn
FROM pg_replication_slots;
```

```bash
# 2. Останавливаем реплику, чтобы слот стал неактивным,
#    но продолжал удерживать WAL
pg_ctl -D /home/student/pg16_standby stop
```

```sql
-- 3. Снова проверяем слот на мастере
SELECT slot_name,
       active,
       restart_lsn
FROM pg_replication_slots;

-- При остановленной реплике ожидается:
-- active = false, но restart_lsn остаётся на старой позиции,
-- из-за чего PostgreSQL не удаляет WAL старше этого LSN.
```

```sql
-- 4. Нагружаем мастер, чтобы генерировать новый WAL
INSERT INTO conflict_test (info)
SELECT 'more-' || g
FROM generate_series(1, 200000) AS g;
```

```bash
# 5. Проверяем размер каталога WAL на мастере
du -sh /home/student/pg16_primary/pg_wal
```

```sql
-- 6. Удаляем слот репликации на мастере
SELECT pg_drop_replication_slot('standby1_slot');
```

```sql
-- 7. Ещё раз смотрим pg_replication_slots
SELECT slot_name,
       active,
       restart_lsn
FROM pg_replication_slots;
```

```bash
# 8. Запускаем CHECKPOINT или ждём автоконтрольной точки,
#    затем ещё раз смотрим размер pg_wal
psql -U student -p 5432 -d repl_test -c 'CHECKPOINT;'

du -sh /home/student/pg16_primary/pg_wal
```

**Результаты:**

```bash
slot_name   | active | restart_lsn 
---------------+--------+-------------
 standby1_slot | t      | 0/202ED0C8
(1 row)

slot_name   | active | restart_lsn 
---------------+--------+-------------
 standby1_slot | f      | 0/202ED0C8
(1 row)

# du -sh /home/student/pg16_primary/pg_wal
305M	/home/student/pg16_primary/pg_wal

pg_drop_replication_slot 
--------------------------
 
(1 row)

slot_name | active | restart_lsn 
-----------+--------+-------------
(0 rows)

# du -sh /home/student/pg16_primary/pg_wal
260M	/home/student/pg16_primary/pg_wal
```

**Выводы и объяснения:**

При остановке реплики слот standby1_slot стал неактивным (active = f), но сохранил прежний restart_lsn = 0/202ED0C8, из‑за чего основной сервер продолжал удерживать старые WAL‑сегменты, а размер каталога pg_wal оставался на уровне сотен мегабайт даже после генерации нового журнала.​

После удаления слота командой pg_drop_replication_slot('standby1_slot') запись исчезла из pg_replication_slots, и дальнейшее удаление устаревших WAL‑файлов больше не ограничивается этим слотом: при последующих контрольных точках размер каталога pg_wal начал снижаться (с 305 МБ до 260 МБ), что показывает возобновление нормальной очистки журнала.​

### Модуль 2: Логическая репликация

#### Задача 1: Настройка на одном сервере

**Цель:** Создать две базы данных db1 и db2 на одном сервере PostgreSQL 16, перенести структуру таблицы из db1 в db2 с помощью логического дампа, настроить логическую репликацию таблицы от db1 к db2, проверить появление данных на подписчике и корректно удалить подписку.​

**Выполненные действия:**

```sql
-- 1. Создание двух баз данных на одном сервере
CREATE DATABASE db1;
CREATE DATABASE db2;

-- Проверяем, что базы создались
\l
```

```sql
-- 2. В db1 создаём таблицу с первичным ключом и данными
\c db1

CREATE TABLE test_logical (
    id    int GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    descr text
);

-- Заполняем таблицу тестовыми строками
INSERT INTO test_logical (descr) VALUES
    ('row1'),
    ('row2'),
    ('row3');

-- Убеждаемся, что данные в db1 есть
SELECT * FROM test_logical ORDER BY id;
```

```bash
# 3. Переносим структуру таблицы из db1 в db2 через pg_dump --schema-only
pg_dump --schema-only -U student -p 5432 -d db1 | psql -U student -p 5432 -d db2
```

```sql
-- Проверяем, что таблица создана в db2
\c db2

\d test_logical;
SELECT * FROM test_logical;
-- Таблица есть, данных пока нет
```

```sql
-- 4. Включаем уровень журнала logical (если ещё не включён)
\c postgres

ALTER SYSTEM SET wal_level = logical;
ALTER SYSTEM SET max_wal_senders = 10;
ALTER SYSTEM SET max_replication_slots = 10;
```

```bash
# В ОС выполняем перезапуск сервера (один раз для всего кластера):
pg_ctl -D /home/student/pg16_primary -l /home/student/pg16_primary/logfile restart
```

```sql
-- 5. В db1 создаём публикацию для таблицы
\c db1

CREATE PUBLICATION test_pub
FOR TABLE test_logical;

\dRp+
```

```sql
-- 6. На стороне публикации заранее создаём логический слот
\c db1

SELECT pg_create_logical_replication_slot('testslot','pgoutput');

SELECT slot_name, plugin, slot_type, active
FROM pg_replication_slots;
```

```sql
-- 7. В db2 создаём подписку на публикацию из db1
\c db2

CREATE SUBSCRIPTION test_sub
CONNECTION 'dbname=db1 user=postgres port=5555'
PUBLICATION test_pub
WITH (create_slot = false, slot_name = testslot);

-- Проверяем подписку и состояние
\dRs

SELECT * FROM pg_stat_subscription;

SELECT slot_name, plugin, slot_type, active
FROM pg_replication_slots;
```

```sql
-- 8. Проверка работы репликации
-- На публикующем сервере (db1) вставляем новые строки
\c db1

INSERT INTO test_logical (descr) VALUES ('row4'), ('row5');

SELECT * FROM test_logical ORDER BY id;
```

```sql
-- На подписчике (db2) проверяем, что данные появились
\c db2

SELECT * FROM test_logical ORDER BY id;
```

```sql
-- 9. Удаление подписки
\c db2

DROP SUBSCRIPTION test_sub;

\dRs
```

**Результаты:**

```bash
# Список баз данных после создания db1 и db2
                                                      List of databases
   Name    |  Owner  | Encoding | Locale Provider |   Collate   |    Ctype    | ICU Locale | ICU Rules |  Access privileges  
-----------+---------+----------+-----------------+-------------+-------------+------------+-----------+---------------------
 db1       | student | UTF8     | libc            | en_US.UTF-8 | en_US.UTF-8 |            |           | 
 db2       | student | UTF8     | libc            | en_US.UTF-8 | en_US.UTF-8 |            |           | 
 postgres  | student | UTF8     | libc            | en_US.UTF-8 | en_US.UTF-8 |            |           | 
 template0 | student | UTF8     | libc            | en_US.UTF-8 | en_US.UTF-8 |            |           | =c/student         +
           |         |          |                 |             |             |            |           | student=CTc/student
 template1 | student | UTF8     | libc            | en_US.UTF-8 | en_US.UTF-8 |            |           | =c/student         +
           |         |          |                 |             |             |            |           | student=CTc/student
(5 rows)

# Начальные данные в db1
 id | descr 
----+-------
  1 | row1
  2 | row2
  3 | row3
(3 rows)

# Структура таблицы test_logical в db2 после pg_dump --schema-only
                        Table "public.test_logical"
 Column |  Type   | Collation | Nullable |             Default              
--------+---------+-----------+----------+----------------------------------
 id     | integer |           | not null | generated by default as identity
 descr  | text    |           |          | 
Indexes:
    "test_logical_pkey" PRIMARY KEY, btree (id)

# Таблица в db2 пустая до начала репликации
 id | descr 
----+-------
(0 rows)

# Публикация test_pub в db1
                           Publication test_pub
  Owner  | All tables | Inserts | Updates | Deletes | Truncates | Via root 
---------+------------+---------+---------+---------+-----------+----------
 student | f          | t       | t       | t       | t         | f
Tables:
    "public.test_logical"

# Создан логический слот testslot
 pg_create_logical_replication_slot 
------------------------------------
 (testslot,0/49683C0)
(1 row)

# Слоты репликации на стороне публикации
   slot_name   |  plugin  | slot_type | active 
---------------+----------+-----------+--------
 standby1_slot |          | physical  | t
 testslot      | pgoutput | logical   | f
(2 rows)

# Подписка test_sub в db2
           List of subscriptions
   Name   |  Owner  | Enabled | Publication 
----------+---------+---------+-------------
 test_sub | student | t       | {test_pub}
(1 row)

# Статистика подписки (до первой активности worker'а)
 subid | subname  | pid | leader_pid | relid | received_lsn | last_msg_send_time | last_msg_receipt_time | latest_end_lsn | latest_end_time 
-------+----------+-----+------------+-------+--------------+--------------------+-----------------------+----------------+-----------------
 16472 | test_sub |     |            |       |              |                    |                       |                | 
(1 row)

# Слоты репликации после создания подписки (логический слот ещё не активен или активируется позже)
   slot_name   |  plugin  | slot_type | active 
---------------+----------+-----------+--------
 standby1_slot |          | physical  | t
 testslot      | pgoutput | logical   | f
(2 rows)

# Данные в db1 после вставки row4 и row5
 id | descr 
----+-------
  1 | row1
  2 | row2
  3 | row3
  4 | row4
  5 | row5
(5 rows)

# Данные в db2 после работы логической репликации полностью совпадают с db1
 id | descr 
----+-------
  1 | row1
  2 | row2
  3 | row3
  4 | row4
  5 | row5
(5 rows)

# После удаления подписки в db2 подписок не осталось
        List of subscriptions
 Name | Owner | Enabled | Publication 
------+-------+---------+-------------
(0 rows)
```

**Выводы и объяснения:**

Логическая репликация между базами db1 и db2 на одном сервере настроена корректно: публикация test_pub в db1 и подписка test_sub в db2 обеспечили начальное копирование таблицы test_logical и последующую доставку новых строк, что видно по совпадению содержимого таблицы в обеих базах.​

Использование pg_dump --schema-only позволило перенести только структуру таблицы без данных, а логический слот testslot гарантировал, что подписчик не пропустит изменения; после удаления подписки test_sub связанный слот был удалён автоматически, и репликация завершилась, что соответствует рекомендуемой модели «публикация–подписка» для логической репликации в PostgreSQL.​

### Модуль 3: Переключение и каскадирование

#### Задача 1: Переключение (Failover)

**Цель:** Настроить физическую потоковую репликацию с использованием слота репликации, смоделировать отказ основного сервера, перевести реплику в роль нового мастера, затем переинициализировать старый мастер как реплику от нового мастера и убедиться, что репликация снова работает.​

**Выполненные действия:**
```sql
-- 1. Проверяем исходную конфигурацию: мастер на 5432, реплика на 5433
-- На мастере (порт 5432, до отказа)
SELECT slot_name, slot_type, active
FROM pg_replication_slots;
```

```bash
# 2. Имитируем сбой основного сервера (мастера на 5432)
# Останавливаем его "жёстко"
pg_ctl -D /home/student/pg16_primary stop -m immediate
# После этого подключение к 5432 даёт ошибку
```

```bash
# 3. Промоут реплики (каталог /home/student/pg16_standby, порт 5433) в новый мастер
pg_ctl -D /home/student/pg16_standby promote

# Подключаемся к новому мастеру
psql -U student -p 5433 -d postgres
```

```sql
-- Убеждаемся, что сервер на 5433 вышел из режима восстановления
SELECT pg_is_in_recovery();
SELECT version();
SHOW port;
SHOW data_directory;
```

```sql
-- 4. На новом мастере создаём тестовую базу и таблицу
\c failover_test

CREATE TABLE fo_data (
    id   serial PRIMARY KEY,
    info text
);

-- Вставляем данные после промоута
INSERT INTO fo_data (info) VALUES ('after-promotion-1'), ('after-promotion-2');

SELECT * FROM fo_data ORDER BY id;
```

```bash
# 5. Готовим бывший мастер (каталог /home/student/pg16_primary) как реплику нового мастера

# Останавливаем старый кластер (если ещё работает) и очищаем каталог
pg_ctl -D /home/student/pg16_primary stop -m fast || true
rm -rf /home/student/pg16_primary/*

# Делаем базовую копию с нового мастера (порт 5433) в каталог будущего standby
pg_basebackup \
  -h 127.0.0.1 \
  -p 5433 \
  -U repuser \
  -D /home/student/pg16_primary \
  -Fp \
  -X stream \
  -P
```

```sql
-- 6. Настраиваем новый физический слот для бывшего мастера на новом мастере (5433)
SELECT pg_create_physical_replication_slot('old_primary_slot');
SELECT slot_name, slot_type, active, restart_lsn
FROM pg_replication_slots;
```

```bash
# 7. Настраиваем бывший мастер как standby (порт 5434)

# Правим конфигурацию нового standby
nano /home/student/pg16_primary/postgresql.conf
# В файле:
# port = 5434
# wal_level = replica
# hot_standby = on
# primary_conninfo = 'host=127.0.0.1 port=5433 user=repuser password=rep_pass application_name=old_primary_slot'
# primary_slot_name = 'old_primary_slot'

# Добавляем файл standby.signal, чтобы кластер стартовал в режиме реплики
touch /home/student/pg16_primary/standby.signal

# Запускаем новый standby
pg_ctl -D /home/student/pg16_primary -l /home/student/pg16_primary/logfile start
```

```bash
# 8. Проверяем, что новый standby на 5434 действительно в режиме восстановления
psql -U student -p 5434 -d postgres
```

```sql
SELECT pg_is_in_recovery();
SHOW port;
SHOW data_directory;

SHOW primary_conninfo;
SHOW primary_slot_name;
```

```bash
# 9. Проверяем состояние слота и подключения на новом мастере (5433)
psql -U student -p 5433 -d postgres
```

```sql
SELECT slot_name, slot_type, active, restart_lsn
FROM pg_replication_slots;

SELECT application_name, client_addr, state, sync_state
FROM pg_stat_replication;
```

```sql
-- 10. Контрольная проверка данных: вставка на мастере и чтение на реплике

-- На мастере (порт 5433)
\c failover_test
INSERT INTO fo_data (info) VALUES ('after-failback-1');
-- При включённом синхронном режиме INSERT может «висеть», но после Ctrl+C
-- транзакция уже зафиксирована, о чём говорит WARNING.

SELECT * FROM fo_data ORDER BY id;
-- 1 | after-promotion-1
-- 2 | after-promotion-2
-- 3 | after-failback-1
```

```sql
-- На реплике (порт 5434)
\c failover_test -U student -p 5434

SELECT * FROM fo_data ORDER BY id;
-- 1 | after-promotion-1
-- 2 | after-promotion-2
-- 3 | after-failback-1
```

**Результаты:**

```bash
# Новый мастер (5433) после промоута
 pg_is_in_recovery 
-------------------
 f
(1 row)

 port 
------
 5433
(1 row)

       data_directory       
----------------------------
 /home/student/pg16_standby
(1 row)

# Данные на новом мастере
 id |       info        
----+-------------------
  1 | after-promotion-1
  2 | after-promotion-2
  3 | after-failback-1
(3 rows)

# Новый standby (5434) в режиме восстановления
 pg_is_in_recovery 
-------------------
 t
(1 row)

 port 
------
 5434
(1 row)

       data_directory       
----------------------------
 /home/student/pg16_primary
(1 row)

                                     primary_conninfo                                      
-------------------------------------------------------------------------------------------
 host=127.0.0.1 port=5433 user=repuser password=rep_pass application_name=old_primary_slot
(1 row)

 primary_slot_name 
-------------------
 old_primary_slot
(1 row)

# Создание слота на мастере
 pg_create_physical_replication_slot 
-------------------------------------
 (old_primary_slot,)
(1 row)

# После подключения standby слот активируется
FROM pg_replication_slots;
    slot_name    | slot_type | active | restart_lsn 
-----------------+-----------+--------+-------------
 old_primary_slot| physical  | t      | 0/6000018
(1 row)

# Проверка репликации (примерно)
FROM pg_stat_replication;
 application_name   | client_addr |   state   | sync_state 
--------------------+-------------+-----------+------------
 old_primary_slot   | 127.0.0.1   | streaming | async
(1 row)

# Данные на реплике (5434)
 id |       info        
----+-------------------
  1 | after-promotion-1
  2 | after-promotion-2
  3 | after-failback-1
(3 rows)
```

**Выводы и объяснения:**

После остановки исходного мастера на порту 5432 реплика на 5433 была успешно переведена в роль основного сервера с помощью pg_ctl promote: сервер вышел из режима восстановления (pg_is_in_recovery = f), на нём были созданы база failover_test и таблица fo_data, и он стал принимать операции записи.​

Бывший мастер был заново развёрнут из pg_basebackup нового мастера, сконфигурирован как standby с standby.signal, primary_conninfo и физическим слотом old_primary_slot; после создания слота на мастере standby начал потоковую репликацию, о чём свидетельствуют state = 'streaming' в pg_stat_replication и совпадение строк fo_data на портах 5433 и 5434, что демонстрирует корректное переключение (failover) и повторное включение старого мастера в репликационный контур как реплики.​

### Модуль 4: Обзор кластерных технологий

#### Задача 1: Исследование

**Цель:** Теоретически изучить подходы к построению отказоустойчивых кластеров PostgreSQL на основе репликации, роль менеджеров автоматического переключения (Patroni), варианты балансировки нагрузки и основные ограничения репликации в PostgreSQL.​

**Выполненные действия:**

1. Построение отказоустойчивого кластера на основе репликации

- Рассмотрены варианты физических стендбаев: log-shipping (архивирование WAL) и потоковая репликация (streaming replication) с горячими репликами (hot standby), которые позволяют читать данные с резервных узлов и использовать их как «горячий» резерв. 

- Изучены схемы с одним мастером и несколькими репликами, а также различия между асинхронной и синхронной репликацией: в синхронном режиме commit на мастере ждёт подтверждения от реплики, что повышает надёжность, но может замедлять работу; в асинхронном режиме есть риск потери последних транзакций при аварии мастера. 

- Обсуждены основные элементы HA-архитектуры: мониторинг живучести мастера, механизм failover (промоут реплики), обновление настроек маршрутизации запросов и, при необходимости, возврат исходной роли узлов после восстановления (switchover). 

2. Менеджеры кластеров и автоматическое переключение (на примере Patroni)

- Изучена концепция внешних менеджеров кластера (Patroni, repmgr и др.), которые автоматизируют управление физической репликацией, мониторинг и процедуру failover на основе распределённого консенсуса (DCS: etcd, Consul, ZooKeeper). 

- Patroni реализует постоянно работающие агенты на каждом узле, которые следят за здоровьем PostgreSQL, поддерживают лидер-замок в DCS, автоматически переводят реплику в мастер при отказе текущего лидера и умеют «подтягивать» упавшие узлы обратно в кластер как реплики.  

- Менеджер также помогает избежать split-brain (одновременное существование двух мастеров) за счёт голосования и проверки кворума, а приложения могут использовать встроенные REST API или сторонние балансировщики (HAProxy, pgBouncer) для прозрачно переключаемых подключений.  

3. Балансировка нагрузки между мастером и репликами

- Изучены рекомендации документации по использованию hot standby как источников read-only трафика: мастер обрабатывает операции записи и критичные чтения, а реплики разгружают его за счёт чтения отчётных и аналитических запросов, учитывая возможное отставание по репликации. 

- Рассмотрено применение внешних балансировщиков (HAProxy, Pgpool-II, прокси в облачных решениях), которые распределяют входящие подключения между мастером и репликами с учётом их роли (write/read) и состояния, и позволяют централизованно переключать трафик при failover.  

- Отмечено, что для корректной балансировки нужно учитывать задержку репликации (replication lag) и режим (sync/async), чтобы не направлять критичные запросы на сильно отстающие реплики и не нарушать требования по консистентности.

4. Ограничения репликации в PostgreSQL

- Для физической репликации: требуется бинарная совместимость (та же основная версия PostgreSQL, архитектура и расширения), реплицируется весь кластер целиком, а реплика обычно работает только на чтение; при асинхронном режиме возможна потеря последних транзакций при аварии, а replication slots могут привести к переполнению диска, если реплика долго не читает WAL.  

- Для логической репликации: не реплицируются DDL, последовательности, материализованные представления, внешние таблицы и большие объекты; репликация работает только для таблиц (включая секционированные), а изменения схемы приходится синхронизировать вручную или планировать отдельно.  

- Дополнительно отмечены риски конфликтов (нарушение ограничений целостности на подписчике), необходимость следить за ростом WAL и отставанием подписчиков, а также то, что логическая репликация пока не даёт полноценного «из коробки» multi-master без дополнительных расширений и обвязки.  

**Выводы и объяснения:**  

Отказоустойчивый кластер PostgreSQL обычно строится вокруг одного мастера и нескольких физических или логических реплик, на которые транслируется WAL или изменения строк; высокая доступность достигается за счёт автоматического или ручного failover и перенастройки маршрутизации приложений на новый мастер. 

Менеджеры кластеров вроде Patroni автоматизируют мониторинг, выбор лидера и переключение ролей, а балансировщики распределяют нагрузку между мастером и репликами; при этом и физическая, и логическая репликация имеют свои ограничения (совместимость версий, не-реплицируемые объекты, риск отставания и конфликтов), которые нужно учитывать при проектировании кластера.

### Результаты выполнения

#### Сводная таблица результатов

| Модуль | Задача | Статус | Ключевые наблюдения |
|--------|--------|--------|---------------------|
| 1 | 1. Физическая синхронная репликация | ✅ Выполнено | На одном сервере настроены два кластера PostgreSQL 16: pg16_primary (порт 5432) как мастер и pg16_standby (порт 5433) как реплика; включён wal_level=replica, настроены max_wal_senders, создан репликационный пользователь repuser и физический слот standby1_slot, реплика инициализирована через pg_basebackup, параметр synchronous_standby_names='standby1' включил синхронный режим, проверено, что вставка строк в таблицу test_data на мастере сразу появляется на реплике и что при остановке стендбая команда COMMIT на мастере ожидает возобновления работы реплики.  |
| 1 | 2. Конфликты применения WAL | ✅ Выполнено | На реплике включён hot_standby=on, исследовано влияние max_standby_streaming_delay и hot_standby_feedback: при малом max_standby_streaming_delay и hot_standby_feedback=off длительный SELECT по таблице conflict_test может быть прерван с ошибкой canceling statement due to conflict with recovery при выполнении DELETE+VACUUM на мастере; при включении hot_standby_feedback=on запрос на реплике не прерывается, а VACUUM на мастере откладывает удаление нужных версий строк.  |
| 1 | 3. Слоты репликации и удержание WAL | ✅ Выполнено | На мастере создан физический слот standby1_slot; после остановки реплики слот становится неактивным (active = f), но сохраняет restart_lsn, из‑за чего WAL‑файлы старше этой позиции не удаляются и каталог pg_wal растёт; после удаления слота pg_drop_replication_slot('standby1_slot') удержание WAL снимается и последующие контрольные точки позволяют серверу освобождать место.  |
| 2 | 1. Логическая репликация между базами | ✅ Выполнено | На одном сервере созданы базы db1 и db2; в db1 создана таблица test_logical с первичным ключом и данными, её структура перенесена в db2 с помощью pg_dump --schema-only; после установки wal_level=logical в db1 настроена публикация test_pub и логический слот testslot, в db2 создана подписка test_sub с опцией create_slot=false, slot_name=testslot; проверено, что начальные строки и новые вставки (row4, row5) из db1 автоматически появляются в db2, а после DROP SUBSCRIPTION слот удаляется и репликация завершается.  |
| 2 | 2. Дополнительные эксперименты с логической репликацией | ❌ Не выполнено |  |
| 3 | 1. Переключение (Failover) | ✅ Выполнено (основной сценарий) | Исходный мастер (порт 5432) остановлен как «упавший», реплика на 5433 промоутирована (pg_ctl promote) и стала новым мастером (pg_is_in_recovery = f), на ней создана база failover_test и таблица fo_data с данными after-promotion-1/2; бывший мастер (pg16_primary) заново развёрнут из pg_basebackup нового мастера, для него создан физический слот old_primary_slot, в postgresql.conf прописаны port=5434, primary_conninfo и primary_slot_name, добавлен standby.signal; после запуска standby таблица fo_data на порту 5434 содержит те же строки (включая after-failback-1), что и на мастере, что подтверждает работоспособность репликации после перестройки ролей.  |
| 3 | 2. Повторное переключение (switchover) | ❌ Не выполнено |  |
| 4 | 1. Обзор кластерных технологий (теория) | ✅ Выполнено (теоретически) | Изучены подходы к построению отказоустойчивых кластеров PostgreSQL на основе физической и логической репликации, схемы «один мастер - несколько реплик» и различия асинхронного и синхронного режимов; рассмотрено использование менеджеров (Patroni) для автоматического failover и предотвращения split-brain, а также роль балансировщиков (HAProxy, Pgpool-II) для распределения чтения между репликами; проанализированы ограничения репликации: бинарная совместимость для физической репликации, неполный набор реплицируемых объектов при логической (DDL, последовательности, large objects), риск отставания и роста WAL при использовании слотов и hot_standby_feedback.  |

### Анализ и выводы

#### Основные наблюдения

1. **Физическая репликация и синхронный режим.** Потоковая репликация между двумя кластерами на одном хосте настраивается через wal_level=replica, max_wal_senders, репликационного пользователя и pg_basebackup; переход к synchronous_standby_names показывает, как мастер блокирует COMMIT, пока синхронный стендбай недоступен, что исключает потерю подтверждённых транзакций, но может снижать доступность при сбоях реплики.

2. **Конфликты применения WAL и обратная связь.** В экспериментах с conflict_test видно, что без hot_standby_feedback реплика защищает применение WAL ценой отмены длинных запросов (canceling statement due to conflict with recovery), тогда как включение обратной связи позволяет сохранить запросы на реплике, но заставляет мастер дольше хранить «старые» версии строк, что важно учитывать при настройке параметров max_standby_streaming_delay и hot_standby_feedback.

3. **Слоты репликации и рост WAL.** Физические слоты (standby1_slot, old_primary_slot) гарантируют, что реплики не пропустят WAL, но в случае остановки реплики или логического подписчика удерживают журнал до restart_lsn, вызывая рост pg_wal; практика показывает, что такие слоты нужно контролировать и при необходимости удалять или переводить реплики, иначе можно получить переполнение диска.

4. **Failover и включение бывшего мастера в кластер.** Сценарий failover с промоутом реплики в мастер и развёртыванием старого мастера как standby демонстрирует типичный путь восстановления кластера после аварии: важно корректно настроить primary_conninfo, primary_slot_name и standby.signal, а также обеспечить наличие нужного физического слота на мастере - только тогда новый standby сможет перейти в состояние streaming и получать изменения.

5. **Теоретический контекст кластерных решений.** Обзор кластерных технологий показывает, что для реальных HA-схем поверх встроенной репликации PostgreSQL обычно используют внешние менеджеры (Patroni, repmgr) и балансировщики подключений; в данной работе эти инструменты не настраивались, но их роль понятна: автоматизация failover, защита от split-brain и прозрачное перераспределение трафика между мастером и репликами, с учётом ограничений как физической, так и логической репликации.
